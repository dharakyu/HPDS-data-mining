{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script runs a number of experiments on the dataset as described below, mainly\n",
    "sentiment analysis of the context that provided terms appeared in.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_reviews = pd.read_pickle(\"feature_extracted_reviews.pkl\")\n",
    "elems = [\" chair\", \" table\", \" sofa\", \" couch\", \" art\", \" window\", \" decor \", \" wall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. what words immediately precede feature words?\n",
    "freqs = []\n",
    "\n",
    "for elem in elems:\n",
    "    pre_words = []\n",
    "    rel_reviews = tagged_reviews.text[tagged_reviews.text.str.contains(elem)]\n",
    "    for review in rel_reviews:\n",
    "        idx = review.find(elem)\n",
    "        end_idx = idx - 2\n",
    "        beg_idx = end_idx\n",
    "        while(review[beg_idx] != \" \"):\n",
    "            beg_idx -= 1\n",
    "\n",
    "        pre_words.append(review[beg_idx: end_idx+2])\n",
    "        \n",
    "    c = Counter(pre_words)\n",
    "    freqs.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what is the specific context (sentence) in which the key features are being mentioned?\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "contexts = []\n",
    "for elem in elems:\n",
    "    elem_contexts = []\n",
    "    rel_reviews = tagged_reviews.text[tagged_reviews.text.str.contains(elem)]\n",
    "    for review in rel_reviews:\n",
    "        sentences = sent_tokenize(review.decode('utf8'))\n",
    "        #display(sentences)\n",
    "        for sentence in sentences:\n",
    "            if elem in sentence:\n",
    "                elem_contexts.append(sentence)\n",
    "                \n",
    "    contexts.append(elem_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates the generated contexts into a dataframe where the columns are the words investigated\n",
    "contexts_df = pd.DataFrame()\n",
    "idx = 0\n",
    "\n",
    "for elem in elems:\n",
    "    col_df = pd.DataFrame(data=contexts[idx], columns=[elem])\n",
    "    contexts_df = pd.concat([contexts_df, col_df], axis=1)\n",
    "    idx += 1\n",
    "    \n",
    "contexts_df.to_pickle(\"./feature_contexts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count1 = 0\n",
    "for line in contexts[0]:\n",
    "    if any(word in line for word in [\" plastic\"]):\n",
    "        count += 1\n",
    "    if any(word in line for word in [\" light\"]):\n",
    "        count1 += 1\n",
    "        \n",
    "print count/float(len(contexts[0]))\n",
    "print count1/float(len(contexts[5]))\n",
    "print len(contexts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in contexts[7]:\n",
    "    print line\n",
    "    print \"-------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of businesses that mention the selected word\n",
    "places = []\n",
    "for line in tagged_reviews.business_name[tagged_reviews.text.str.contains(\" window\")]:\n",
    "    places.append(line)\n",
    "    \n",
    "c = Counter(places)\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the average sentiment of the contexts where the selected word appears\n",
    "# the word itself isn't removed from the sentence when analyzing\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "com = []\n",
    "for sentence in contexts_df[\" window\"].dropna():\n",
    "    #print sentence \n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    neg.append(ss[\"neg\"])\n",
    "    pos.append(ss[\"pos\"])\n",
    "    neu.append(ss[\"neu\"])\n",
    "    com.append(ss[\"compound\"])\n",
    "    \n",
    "print \"negative\", np.mean(neg)\n",
    "print \"positive\", np.mean(pos)\n",
    "print \"neutral\", np.mean(neu)\n",
    "print \"compound\", np.mean(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the average sentiment of all the sentences in all the reviews\n",
    "# this can be used as a baseline instead of 0 because the dataset is biased\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "com = []\n",
    "for review in tagged_reviews.text:\n",
    "    #print sentence \n",
    "    sentences = sent_tokenize(review.decode('utf8'))\n",
    "    for sentence in sentences:\n",
    "        #if \" the\" in sentence:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        neg.append(ss[\"neg\"])\n",
    "        pos.append(ss[\"pos\"])\n",
    "        neu.append(ss[\"neu\"])\n",
    "        com.append(ss[\"compound\"])\n",
    "    \n",
    "print \"negative\", np.mean(neg)\n",
    "print \"positive\", np.mean(pos)\n",
    "print \"neutral\", np.mean(neu)\n",
    "print \"compound\", np.mean(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process for the word 'wonderful'\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "com = []\n",
    "for review in tagged_reviews.text[tagged_reviews.text.str.contains(\" wonderful\")]:\n",
    "    #print sentence \n",
    "    sentences = sent_tokenize(review.decode('utf8'))\n",
    "    for sentence in sentences:\n",
    "        if \" wonderful\" in sentence:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            neg.append(ss[\"neg\"])\n",
    "            pos.append(ss[\"pos\"])\n",
    "            neu.append(ss[\"neu\"])\n",
    "            com.append(ss[\"compound\"])\n",
    "    \n",
    "print \"negative\", np.mean(neg)\n",
    "print \"positive\", np.mean(pos)\n",
    "print \"neutral\", np.mean(neu)\n",
    "print \"compound\", np.mean(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process for the word 'plastic'\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "com = []\n",
    "for review in tagged_reviews.text[tagged_reviews.text.str.contains(\" plastic \")]:\n",
    "    #print sentence \n",
    "    sentences = sent_tokenize(review.decode('utf8'))\n",
    "    for sentence in sentences:\n",
    "        if \" plastic \" in sentence:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            neg.append(ss[\"neg\"])\n",
    "            pos.append(ss[\"pos\"])\n",
    "            neu.append(ss[\"neu\"])\n",
    "            com.append(ss[\"compound\"])\n",
    "    \n",
    "print \"negative\", np.mean(neg)\n",
    "print \"positive\", np.mean(pos)\n",
    "print \"neutral\", np.mean(neu)\n",
    "print \"compound\", np.mean(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of terms to be investigated in a similar process\n",
    "feature_words = [\" chair\", \" table\", \" sofa\", \" couch\", \" art\", \" window\", \" decor \", \" wall\", \" natural light\", \n",
    "                 \" water\", \" fountain\", \" stairs\", \" staircase\", \" internet\", \" local art\", \" music\", \n",
    "                \" big table\", \" small table\"]\n",
    "adjectives = [\" cozy\", \" comfy\", \" comfortable\", \" clean\", \" dirty\", \" nois\", \" quiet\"]\n",
    "keywords = feature_words + adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average sentimnet of all the sentences in the corpus that contained each\n",
    "# of the terms provided above\n",
    "compound_scores = []\n",
    "\n",
    "for word in keywords:\n",
    "    #neg = []\n",
    "    #pos = []\n",
    "    #neu = []\n",
    "    com = []\n",
    "    for review in tagged_reviews.text[tagged_reviews.text.str.contains(word)]:\n",
    "        sentences = sent_tokenize(review.decode('utf8'))\n",
    "        for sentence in sentences:\n",
    "            if word in sentence:\n",
    "                ss = sid.polarity_scores(sentence)\n",
    "                #neg.append(ss[\"neg\"])\n",
    "                #pos.append(ss[\"pos\"])\n",
    "                #neu.append(ss[\"neu\"])\n",
    "                com.append(ss[\"compound\"])\n",
    "\n",
    "    compound_scores.append(np.mean(com))\n",
    "    \n",
    "\n",
    "display(compound_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these sentiments\n",
    "y_pos = [i for i, _ in enumerate(keywords)]\n",
    "\n",
    "plt.barh(y_pos, compound_scores)\n",
    "plt.ylabel(\"word\")\n",
    "plt.xlabel(\"compound score\")\n",
    "\n",
    "plt.yticks(y_pos, keywords)\n",
    "plt.axvline(x=0.2809892771562327, color='red')\n",
    "plt.xlim(-0.3, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control analysis: in this test, we follow the same earlier procedures, except that we\n",
    "# replace the terms with neutral ones according to whether they are nouns or adjectives\n",
    "# This is to cancel the effect of the word itself on the sentiment when we care more about the context\n",
    "compound_scores1 = []\n",
    "\n",
    "# nouns\n",
    "for word in feature_words:\n",
    "    #neg = []\n",
    "    #pos = []\n",
    "    #neu = []\n",
    "    com = []\n",
    "    for review in tagged_reviews.text[tagged_reviews.text.str.contains(word)]:\n",
    "        sentences = sent_tokenize(review.decode('utf8'))\n",
    "        for sentence in sentences:\n",
    "            if word in sentence:\n",
    "                ss = sid.polarity_scores(sentence.replace(word, \" chair\"))\n",
    "                neg.append(ss[\"neg\"])\n",
    "                pos.append(ss[\"pos\"])\n",
    "                neu.append(ss[\"neu\"])\n",
    "                com.append(ss[\"compound\"])\n",
    "\n",
    "    compound_scores1.append(np.mean(com))\n",
    "    \n",
    "# adjectives\n",
    "for word in adjectives:\n",
    "    #neg = []\n",
    "    #pos = []\n",
    "    #neu = []\n",
    "    com = []\n",
    "    for review in tagged_reviews.text[tagged_reviews.text.str.contains(word)]:\n",
    "        sentences = sent_tokenize(review.decode('utf8'))\n",
    "        for sentence in sentences:\n",
    "            if word in sentence:\n",
    "                ss = sid.polarity_scores(sentence.replace(word, \" blue\"))\n",
    "                neg.append(ss[\"neg\"])\n",
    "                pos.append(ss[\"pos\"])\n",
    "                neu.append(ss[\"neu\"])\n",
    "                com.append(ss[\"compound\"])\n",
    "\n",
    "    compound_scores1.append(np.mean(com))\n",
    "\n",
    "display(compound_scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the control analysis\n",
    "y_pos = [i for i, _ in enumerate(keywords)]\n",
    "\n",
    "plt.barh(y_pos, compound_scores)\n",
    "plt.ylabel(\"word\")\n",
    "plt.xlabel(\"compound score\")\n",
    "\n",
    "plt.yticks(y_pos, keywords)\n",
    "plt.axvline(x=0.2809892771562327, color='red')\n",
    "plt.xlim(-0.3, 0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both of the earlier plots + the counts of sentences that support them\n",
    "counts = []\n",
    "for word in keywords:\n",
    "    counts.append(tagged_reviews.text[tagged_reviews.text.str.contains(word)].count())\n",
    "    \n",
    "print counts\n",
    "\n",
    "y_pos = [i for i, _ in enumerate(keywords)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35\n",
    "\n",
    "p1 = ax.bar(y_pos, compound_scores, width)\n",
    "p2 = ax.bar(np.add(y_pos, width), compound_scores1, width)\n",
    "\n",
    "ax.set_title('Average sentiment of sentences containing key words')\n",
    "ax.set_ylabel('words')\n",
    "ax.set_xlabel('sentiment')\n",
    "\n",
    "ax.legend((p1[0], p2[0]), ('Original', 'Replaced'))\n",
    "plt.xticks(y_pos, keywords, rotation='vertical')\n",
    "plt.axhline(y=0.2809892771562327, color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_pos, counts)\n",
    "plt.xticks(y_pos, keywords, rotation='vertical')\n",
    "plt.axhline(y=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces a list of all the words that co-occured with the selected word with counts\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for word in [\" window\"]:\n",
    "    \n",
    "    for review in tagged_reviews.text[tagged_reviews.text.str.contains(word)]:\n",
    "        sentences = sent_tokenize(review.decode('utf8'))\n",
    "        for sentence in sentences:\n",
    "            if word in sentence:\n",
    "                words = word_tokenize(sentence)\n",
    "                all_words = all_words + words\n",
    "                \n",
    "c = Counter(all_words)\n",
    "display(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude stopwords and punctuation from the prvious product\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "for word in sorted(c, key=c.get, reverse=True):\n",
    "    if word not in stopwords.words('english') and word not in string.punctuation:       \n",
    "        print word, c[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of reviews containing the selected word\n",
    "tagged_reviews.text[tagged_reviews.text.str.contains(\" water\")].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
